{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links: https://github.com/ddz16/TSFpaper, KAN4TSF => https://github.com/2448845600/EasyTSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In recent years, deep neural networks have revolutionized the field of machine learning, achieving remarkable success in various applications. The majority of these architectures are built upon the fundamental concept of the multi-layer perceptron (MLP), which consists of linear layers performing matrix multiplication. Even state-of-the-art models like Transformers, while not strictly MLPs, share this core technology.\n",
    "\n",
    "However, a new type of neural network, inspired by the Kolmogorov-Arnold representation theorem, is emerging as a potential alternative to traditional MLPs. These networks, dubbed Korov-Arnold Networks (KANs), introduce a radically different building block that challenges our understanding of how neural networks operate.\n",
    "\n",
    "The K-representation theorem states that any function can be represented using a specific construction. This is theoretically promising, as it suggests the possibility of representing any desired function. However, as researchers in the 1980s discovered when attempting to build shallow neural networks for complex tasks, theoretical possibility does not guarantee practical trainability. The theory asserted that a shallow network with a single hidden layer could represent anything, but effectively training such a network to perform complex functions proved to be a significant challenge.\n",
    "\n",
    "KANs, while still composed of nodes and connections similar to MLPs, exhibit a key difference in where the critical computations occur. In MLPs, the nodes perform the essential work, combining inputs with varying weights to produce a single output with a nonlinear activation function. Conversely, in KANs, the edges take center stage. Each edge receives input from a node and applies a complex function, such as $x^2$ or other intricate operations, before passing the result to the next node. The nodes in KANs serve a simpler purpose, merely computing an unweighted sum of their inputs.\n",
    "\n",
    "This shift in computational focus from nodes to edges represents a significant departure from traditional neural network architectures. By allowing the edges to perform complex transformations, KANs have the potential to capture intricate relationships and represent functions in a more expressive and efficient manner.\n",
    "\n",
    "As researchers continue to explore the capabilities and limitations of KANs, it remains to be seen whether they will surpass the performance of MLPs and establish themselves as a new standard in neural network design. Nevertheless, the introduction of KANs has sparked excitement and curiosity within the machine learning community, prompting further investigation into this novel approach to building neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "- The report should be roughly in the style of a blog post, including introduction, motivation, what you tried, what worked and didn’t work, etc. \n",
    "- Make sure you evaluate both the good and bad points of your approach.\n",
    "- Show results of at least one experiment evaluating some aspect of or your entire approach, preferably showing error bars or some sort of statistical measure of the significance. Even if you didn't accomplish your goal, evaluate what you did do.\n",
    "- A single well-analyzed experiment in a simple domain that compares clearly against a baseline is preferable to a shallow set of experiments across many domains.\n",
    "- If any parameters are mentioned in the report, be sure to mention how you arrived at their values. Was it the first thing you tried? Trial and error? Roughly how many trials? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The majority of deep learning architectures today are built using multi-layer perceptrons (MLP), which at its core, just consists of linear layers performing matrix multiplication. Even recent advances like Multi-head Attention are still just an extension of this. However, 6 months ago, a new type of neural network, dubbed Korov-Arnold Networks (KANs), started trending on Twitter because it promised to be a radically different approach to building neural networks, with the potential for greater expressivity and efficiency.\n",
    "\n",
    "Akin to the Universal Approximation Theorem that underlies all MLPs [link to youtube video and wiki], the Kolmogorov–Arnold representation theorem similarly states that any function can be represented as a superposition of continuous single-variable functions. This is significant because it suggests that KANs, which are based on this theorem, may be capable of representing a wider range of functions than traditional MLPs. Furthermore, the theorem implies that this representation can be achieved using a relatively simple structure, potentially leading to more compact and efficient models.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"KANs.png\" alt=\"KANs\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "In traditional MLPs, the nodes perform the heavy lifting, combining inputs with learned weights and applying a nonlinear activation function to produce the output. The edges merely serve as conduits for passing information between nodes. In contrast, KANs flip this paradigm on its head. The edges take center stage, applying complex functions to the inputs before passing the results to the next node. The nodes themselves perform a simpler role, simply summing their inputs without any learned weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This shift in computational focus has several potential advantages. By allowing the edges to perform complex transformations, KANs may be able to capture more intricate relationships between inputs and outputs. The learnable activation functions on the edges enable KANs to adapt better to complex data patterns compared to MLPs with fixed activation functions. Additionally, because the nodes in KANs do not require learned parameters, the total number of parameters in the network may be vastly reduced, leading to more efficient models that ideally are less prone to overfitting. Furthermore, due to their structure, KANs can be more interpretable than MLPs, which can be beneficial in applications where understanding the model's decision-making process is important.\n",
    "\n",
    "Unfortunately, KANs also have some drawbacks. One major disadvantage is that KANs are considerably slower to train compared to MLPs, which have benefited from years of optimization and hardware acceleration. This increased computational cost can limit the scalability of KANs to larger and more complex tasks. Additionally, some research indicates [needs citation] that KANs may struggle to perform as well as MLPs on highly complex datasets, potentially limiting their applicability in certain scenarios.\n",
    "\n",
    "However, KANs are still a relatively new and largely unexplored architecture. The experiments performed in the original KAN paper were limited to small-scale toy problems and boring classification tasks. Despite these limitations, the unique properties of KANs make them an intriguing candidate for time series analysis. Time series data often exhibits complex temporal dependencies and non-linear relationships that can be challenging for traditional MLPs to capture effectively. The learnable activation functions on the edges of KANs allow them to adapt to these intricate patterns more easily, potentially leading to improved performance in time series forecasting and anomaly detection tasks.\n",
    "\n",
    "Although some preliminary research has been done on their effectiveness in time series analysis [1, https://arxiv.org/pdf/2405.08790] [2, https://arxiv.org/html/2408.11306v1], it remains inconclusive whether they are a promising candidate yet. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
