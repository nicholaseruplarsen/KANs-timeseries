{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "- The report should be roughly in the style of a blog post, including introduction, motivation, what you tried, what worked and didn’t work, etc. \n",
    "- Make sure you evaluate both the good and bad points of your approach.\n",
    "- Show results of at least one experiment evaluating some aspect of or your entire approach, preferably showing error bars or some sort of statistical measure of the significance. Even if you didn't accomplish your goal, evaluate what you did do.\n",
    "- A single well-analyzed experiment in a simple domain that compares clearly against a baseline is preferable to a shallow set of experiments across many domains.\n",
    "- If any parameters are mentioned in the report, be sure to mention how you arrived at their values. Was it the first thing you tried? Trial and error? Roughly how many trials? etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The majority of deep learning architectures today are built using multi-layer perceptrons (MLP), which at its core, just consists of linear layers performing matrix multiplication. Even recent advances like Multi-head Attention are still just an extension of this. However, 6 months ago, a new type of neural network, dubbed Korov-Arnold Networks (KANs), started trending on Twitter because it promised to be a radically different approach to building neural networks, with the potential for greater expressivity and efficiency.\n",
    "\n",
    "Akin to the Universal Approximation Theorem that underlies all MLPs [link to youtube video and wiki], the Kolmogorov–Arnold representation theorem similarly states that any function can be represented as a superposition of continuous single-variable functions. This is significant because it suggests that KANs, which are based on this theorem, may be capable of representing a wider range of functions than traditional MLPs. Furthermore, the theorem implies that this representation can be achieved using a relatively simple structure, potentially leading to more compact and efficient models.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"KANs.png\" alt=\"KANs\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "In traditional MLPs, the nodes perform the heavy lifting, combining inputs with learned weights and applying a nonlinear activation function to produce the output. The edges merely serve as conduits for passing information between nodes. In contrast, KANs flip this paradigm on its head. The edges take center stage, applying complex functions to the inputs before passing the results to the next node. The nodes themselves perform a simpler role, simply summing their inputs without any learned weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "This shift in computational focus has several potential advantages. By allowing the edges to perform complex transformations, KANs may be able to capture more intricate relationships between inputs and outputs. The learnable activation functions on the edges enable KANs to adapt better to complex data patterns compared to MLPs with fixed activation functions. Additionally, because the nodes in KANs do not require learned parameters, the total number of parameters in the network may be vastly reduced, leading to more efficient models that ideally are less prone to overfitting. Furthermore, due to their structure, KANs can be more interpretable than MLPs, which can be beneficial in applications where understanding the model's decision-making process is important.\n",
    "\n",
    "Unfortunately, KANs also have some drawbacks. One major disadvantage is that KANs are considerably slower to train compared to MLPs, which have benefited from years of optimization and hardware acceleration. This increased computational cost can limit the scalability of KANs to larger and more complex tasks. Additionally, some research indicates [needs citation] that KANs may struggle to perform as well as MLPs on highly complex datasets, potentially limiting their applicability in certain scenarios.\n",
    "\n",
    "However, KANs are still a relatively new and largely unexplored architecture. The experiments performed in the original KAN paper were limited to small-scale toy problems and boring classification tasks. Despite these limitations, the unique properties of KANs make them an intriguing candidate for time series analysis. Time series data often exhibits complex temporal dependencies and non-linear relationships that can be challenging for traditional MLPs to capture effectively. The learnable activation functions on the edges of KANs allow them to adapt to these intricate patterns more easily, potentially leading to improved performance in time series forecasting and anomaly detection tasks.\n",
    "\n",
    "Although some preliminary research has been done on their effectiveness in time series analysis [1, https://arxiv.org/pdf/2405.08790] [2, https://arxiv.org/html/2408.11306v1], it remains inconclusive whether they are a promising candidate yet. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
